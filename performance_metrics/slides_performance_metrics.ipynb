{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Playground for Performance Measure Slides\n",
    "\n",
    "- Stephen W. Thomas\n",
    "- Used for MMA 869, MMAI 869, and GMMA 869"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-11-04 16:09:10.998456\n"
     ]
    }
   ],
   "source": [
    "import datetime\n",
    "print(datetime.datetime.now())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.metrics import silhouette_score, silhouette_samples\n",
    "\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test = np.ones((20,), dtype=int)\n",
    "y_test = np.append(y_test, np.zeros((80,), dtype=int))\n",
    "\n",
    "y_pred = np.ones((18,), dtype=int)\n",
    "y_pred = np.append(y_pred, np.zeros((2,), dtype=int))\n",
    "y_pred = np.append(y_pred, np.zeros((75, ), dtype=int))\n",
    "y_pred = np.append(y_pred, np.ones((5,), dtype=int))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([80, 20], dtype=int64)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "array([77, 23], dtype=int64)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.bincount(y_test)\n",
    "np.bincount(y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[75,  5],\n",
       "       [ 2, 18]], dtype=int64)"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import zero_one_loss, classification_report, accuracy_score, cohen_kappa_score, f1_score, log_loss, confusion_matrix, precision_score, recall_score, balanced_accuracy_score, brier_score_loss, precision_recall_fscore_support\n",
    "\n",
    "# C00 = TN\n",
    "# C10 = FN\n",
    "# C11 = TP\n",
    "# C01 = FP\n",
    "confusion_matrix(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          pred:0  pred:1\n",
      "  true:0      75       5\n",
      "  true:1       2      18\n"
     ]
    }
   ],
   "source": [
    "unique_label = np.unique([y_test, y_pred])\n",
    "cmtx = pd.DataFrame(\n",
    "confusion_matrix(y_test, y_pred, labels=unique_label), \n",
    "index=['  true:{:}'.format(x) for x in unique_label], \n",
    "columns=['pred:{:}'.format(x) for x in unique_label])\n",
    "print(cmtx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy          = 0.930\n",
      "Zero-one Loss     = 0.070\n",
      "Balanced Accuracy = 0.919\n",
      "Brier Score       = 0.070\n",
      "Precision         = 0.783\n",
      "Recall            = 0.900\n",
      "F1 Score          = 0.837\n"
     ]
    }
   ],
   "source": [
    "print(\"Accuracy          = {:.3f}\".format(accuracy_score(y_test, y_pred)))\n",
    "print(\"Zero-one Loss     = {:.3f}\".format(zero_one_loss(y_test, y_pred)))\n",
    "print(\"Balanced Accuracy = {:.3f}\".format(balanced_accuracy_score(y_test, y_pred)))\n",
    "print(\"Brier Score       = {:.3f}\".format(brier_score_loss(y_test, y_pred)))\n",
    "print(\"Precision         = {:.3f}\".format(precision_score(y_test, y_pred)))\n",
    "print(\"Recall            = {:.3f}\".format(recall_score(y_test, y_pred)))\n",
    "print(\"F1 Score          = {:.3f}\".format(f1_score(y_test, y_pred)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(array([0.97402597, 0.7826087 ]), array([0.9375, 0.9   ]), array([0.95541401, 0.8372093 ]), array([80, 20], dtype=int64))\n"
     ]
    }
   ],
   "source": [
    "print(precision_recall_fscore_support(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.94      0.96        80\n",
      "           1       0.78      0.90      0.84        20\n",
      "\n",
      "    accuracy                           0.93       100\n",
      "   macro avg       0.88      0.92      0.90       100\n",
      "weighted avg       0.94      0.93      0.93       100\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Average = binary\n",
      "  Precision = 0.783\n",
      "  Recall    = 0.900\n",
      "  F1        = 0.837\n",
      "\n",
      "Average = micro\n",
      "  Precision = 0.930\n",
      "  Recall    = 0.930\n",
      "  F1        = 0.930\n",
      "\n",
      "Average = macro\n",
      "  Precision = 0.878\n",
      "  Recall    = 0.919\n",
      "  F1        = 0.896\n",
      "\n",
      "Average = weighted\n",
      "  Precision = 0.936\n",
      "  Recall    = 0.930\n",
      "  F1        = 0.932\n"
     ]
    }
   ],
   "source": [
    "averages = ['binary', 'micro', 'macro', 'weighted']\n",
    "\n",
    "for average in averages:\n",
    "    print(\"\\nAverage = {}\".format(average))\n",
    "    print(\"  Precision = {:.3f}\".format(precision_score(y_test, y_pred, average=average)))\n",
    "    print(\"  Recall    = {:.3f}\".format(recall_score(y_test, y_pred, average=average)))\n",
    "    print(\"  F1        = {:.3f}\".format(f1_score(y_test, y_pred, average=average)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
